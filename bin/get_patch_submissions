#!/usr/bin/python

import os
import re
import sys
import urlparse
import mechanize

from datetime import datetime
from argparse import ArgumentParser
from BeautifulSoup import BeautifulSoup

sys.path.insert(0, os.path.realpath(
            os.path.join(os.path.dirname(__file__), "..")))
from autotwiki.week import Week

projects = {
    'cairo': {
        'base_url': "http://lists.cairographics.org/archives/cairo",
    },
    'wayland': {
        'base_url': "http://lists.freedesktop.org/archives/wayland-devel",
    },
}

my_name = "Bryce Harrington"

def get_monthly_archives(br, base_url, num_months=1):
    monthly_archives = []

    r = br.open(base_url)
    html = r.read()
    soup = BeautifulSoup(html)

    # TODO: Determine the appropriate month to load.  For now, just pick
    # the first month listed.

    for n in range(0, num_months):
        row = soup.findAll('tr')[n+1]
        month, year = row.findAll('td')[0].text.replace(':', '').split(' ')
        links = row.findAll('td')[1]
        for link in links.findAll('a'):
            if link.text == "[ Date ]":
                monthly_archives.append((month, year, "%s/%s" %(base_url, link['href'])))
                break

        if len(monthly_archives) == num_months:
            return monthly_archives

    return []

if __name__ == '__main__':
    ### Handle program arguments
    parser = ArgumentParser(description='Get release announcement URLs from mailman archives')
    parser.add_argument(dest='project', type=str, choices=projects.keys())
    args = parser.parse_args()
    base_url = projects[args.project]['base_url']

    br = mechanize.Browser()
    br.set_handle_redirect(True)
    br.set_handle_referer(True)
    br.set_handle_robots(False)
    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)
    br.addheaders = [
        ('User-agent', ('Mozilla/5.0 (X11; U; Linux i686; en-US;'
        ' rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox'
        '/3.0.1'))
        ]

    for month, year, month_archive_url in get_monthly_archives(br, base_url, 3):
        print
        print "%s-%s" %(month, year)
        if month_archive_url is None:
            print "Error: Couldn't find month archive url"
            sys.exit(1)

        month_url = "%s/%s-%s" %(base_url, year, month)

        r = br.open(month_archive_url)
        html = r.read()
        soup = BeautifulSoup(html)
        for item in soup.findAll('li'):
            # Parse the subject
            links = item.findAll('a')
            if len(links) < 1:
                continue
            subject = links[0].text
            if "[PATCH]" not in subject:
                continue

            # Parse the author, make sure it's me
            name = item.findAll('i')
            if len(name) < 1:
                continue
            author = name[0].text
            if author != my_name:
                continue

            # Load the email content itself
            post_url = "%s/%s" %(month_url, links[0]['href'])
            r = br.open(post_url)
            post_html = r.read()

            # Verify the patch is signed off by me
            post_soup = BeautifulSoup(post_html)
            signoffs = post_soup.body.findAll(text=re.compile("\nSigned-off-by: %s" %(author)))
            if len(signoffs) < 1:
                continue

            post_date_str = post_soup.findAll("i")[0].text
            post_date = datetime.strptime(post_date_str, "%a %b %d %H:%M:%S %Z %Y")
            post_week_number = Week(date=post_date).number

            prev_message_subject = post_soup.findAll("li")[0].text
            if subject in prev_message_subject:
                continue

            print "  %s" %(subject)
            print "  >> %s" %(prev_message_subject)
            print "  %s" %(author)
            print "  %s" %(post_url)
            print "  %s" %(post_date)
            print "  Week #%d" %(post_week_number)
            print
